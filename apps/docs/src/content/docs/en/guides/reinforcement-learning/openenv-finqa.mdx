---
title: Multi-Turn RL Training with OpenEnv and Daytona
description: Use parallel Daytona sandboxes with OpenEnv's FinQA environment to train Qwen3-14B on financial question answering via GRPO.
---

import { TabItem, Tabs } from '@astrojs/starlight/components'

{/* import rewardsPlot from '../../../../../assets/docs/images/openenv-finqa-rewards-plot.png' */}
{/* import { Image } from 'astro:assets' */}

This guide demonstrates how to use [OpenEnv](https://github.com/meta-pytorch/OpenEnv) with Daytona sandboxes to train and evaluate language models on [FinQA](https://huggingface.co/datasets/snorkelai/finqa-data), a financial question-answering benchmark based on SEC 10-K filings.

We cover two modes of using the environment: a **single-episode demo** that walks through one complete interaction, and a **full GRPO training loop** that trains `Qwen3-14B` with LoRA across 500 parallel sandboxes.

---

### 1. Workflow Overview

Unlike single-shot code evaluation (as in the [TRL guide](/docs/guides/reinforcement-learning/trl-grpo-training)), this setup uses **multi-turn tool-calling episodes**. The model iteratively explores financial data using SQL queries before submitting a final answer.

Each episode follows this cycle:
1. **Reset**: A sandbox starts a new episode with a random financial question
2. **Explore**: The model calls tools (`get_descriptions`, `get_table_info`, `sql_query`) to discover and query financial tables
3. **Submit**: After gathering enough data, the model calls `submit_answer` with its answer
4. **Reward**: The environment returns a binary reward (1.0 = correct, 0.0 = wrong)

The available tools in the FinQA environment are:

| Tool | Description |
|------|-------------|
| `get_descriptions(company_name)` | List available tables for a company |
| `get_table_info(company_name, table_name)` | Get column names and types |
| `sql_query(query)` | Run a SQL query against the company's 10-K data |
| `submit_answer(answer)` | Submit a final answer (terminates the episode) |

**Single-episode demo (`run.py`)**: Creates one sandbox, runs one episode, observes the reward, tears down.

**Full training (`train.py`)**: Creates 500 sandboxes, collects multi-turn rollouts in parallel with batched vLLM generation, groups episodes by question for GRPO advantage computation, runs policy gradient updates with LoRA, and hot-swaps adapters into vLLM between iterations.

### 2. Setup

#### Clone the Repository

:::note[GPU Requirement]
The full training requires 4 GPUs with 80GB+ VRAM each (2 for vLLM inference, 2 for training). The single-episode demo (`run.py`) does not require a GPU.
:::

Clone the [Daytona repository](https://github.com/daytonaio/daytona.git) and navigate to the example directory:

```bash
git clone https://github.com/daytonaio/daytona.git
cd daytona/guides/python/openenv/finqa
```

#### Create Virtual Environment

:::note[Python Version]
Python 3.10 or higher is required.
:::

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### Install Dependencies

```bash
bash setup.sh
```

This installs:
- `daytona` - Daytona SDK for sandbox management
- `openenv-core` - OpenEnv runtime (WebSocket-based RL environment protocol)
- `openenv-finqa-env` - The FinQA environment client (`FinQAEnv`, `CallToolAction`)
- `python-dotenv` - Environment variable management

For training, also install the training extras:

```bash
pip install -e ".[train]"
```

This adds: `torch`, `transformers`, `vllm`, `peft` (for LoRA), and `numpy`.

#### Configure Environment

Get your Daytona API key from the [Daytona Dashboard](https://app.daytona.io/dashboard/keys) and create a `.env` file:

```bash
DAYTONA_API_KEY=your_daytona_api_key
```

#### Build the Snapshot

Before running any episodes, build a Daytona snapshot that pre-packages the FinQA environment server and dataset:

```bash
python build_snapshot.py
```

The snapshot builder uses Daytona's declarative Image API to create a reproducible container image:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    image = (
        Image.base("ghcr.io/meta-pytorch/openenv-base:latest")
        .workdir("/app/env")
        .run_commands(
            "apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*",
            f"git clone --depth 1 {OPENENV_REPO} /tmp/openenv",
            "cp -r /tmp/openenv/envs/finqa_env/. /app/env/",
            "rm -rf /tmp/openenv",
        )
        .run_commands(
            "pip install --no-cache-dir -e .",
            # Pre-download FinQA dataset from HuggingFace
            'python -c "'
            "from huggingface_hub import snapshot_download; "
            "snapshot_download('snorkelai/finqa-data', repo_type='dataset', local_dir='/app/env/data')"
            '"',
        )
        .env({
            "PYTHONUNBUFFERED": "1",
            "FINQA_DATA_PATH": "/app/env/data",
        })
        .cmd(["sh", "-c", SERVER_CMD])
    )

    snapshot = daytona.snapshot.create(
        CreateSnapshotParams(name="openenv-finqa", image=image),
        on_logs=lambda line: print(f"  {line}"),
    )
    ```
  </TabItem>
</Tabs>

This builds a container with the FinQA server pre-installed and the dataset pre-downloaded, so sandboxes created from this snapshot start in ~2 seconds with no network dependencies.

### 3. Running a Single Episode

The `run.py` script demonstrates the full OpenEnv + Daytona integration in a single episode. Run it with:

```bash
python run.py
```

Let's walk through the key components.

#### Sandbox Creation

The `DaytonaProvider` from OpenEnv wraps the Daytona SDK, creating a sandbox from the pre-built snapshot and waiting for the FinQA server to become healthy:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    from openenv.core.containers.runtime.daytona_provider import DaytonaProvider

    def create_sandbox():
        provider = DaytonaProvider(auto_stop_interval=0, cmd=SERVER_CMD)
        url = provider.start_container(f"snapshot:{SNAPSHOT}")

        provider.wait_for_ready(url, 120)
        return provider, url
    ```
  </TabItem>
</Tabs>

#### Connecting to the Environment

OpenEnv communicates over WebSocket. The `FinQAEnv` client handles the connection, and `env.reset()` starts a new episode with a random question:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    from finqa_env import CallToolAction, FinQAEnv

    async with FinQAEnv(base_url=url) as env:
        await env.reset()

        # Get the question and company for this episode
        state = await env._send_and_receive({"type": "state"})
        data = state.get("data", {})
        question = data.get("current_question", "")
        company = data.get("current_company", "")
    ```
  </TabItem>
</Tabs>

#### Two API Styles

OpenEnv provides two ways to interact with the environment:

**`call_tool()`** — for exploration, returns the raw result with no RL tracking:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    # Discover available tables
    descriptions = await env.call_tool("get_descriptions", company_name=company)
    table_names = json.loads(descriptions)

    # Inspect a table's schema
    table_info = await env.call_tool(
        "get_table_info", company_name=company, table_name=table_names[0]
    )
    ```
  </TabItem>
</Tabs>

**`step()`** — wraps the tool call in an RL-style `StepResult` with `.observation.done` and `.observation.reward`:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    # Run a SQL query (with RL reward/done tracking)
    query = f'SELECT * FROM "{table_names[0]}" LIMIT 5'
    step_result = await env.step(
        CallToolAction(tool_name="sql_query", arguments={"query": query})
    )
    obs = step_result.observation
    print(f"SQL result (done={obs.done}, reward={obs.reward})")

    # Submit a final answer (terminates the episode)
    step_result = await env.step(
        CallToolAction(tool_name="submit_answer", arguments={"answer": "0"})
    )
    obs = step_result.observation
    print(f"Submitted (done={obs.done}, reward={obs.reward})")
    ```
  </TabItem>
</Tabs>

Use `call_tool()` when exploring, and `step()` when you need reward/done signals (e.g., in a training loop).

#### Expected Output

```
Creating sandbox from snapshot 'openenv-finqa'...
Waiting for server health check...
  Server healthy.

Question: What was the total revenue for fiscal year 2023?
Company:  ExampleCorp
Tables:   ['income_statement', 'balance_sheet', 'cash_flow']
Schema:   {"columns": [{"name": "fiscal_year", "type": "INTEGER"}, ...]}

SQL result (done=False, reward=0.0):
  [{"fiscal_year": 2023, "revenue": 45200, ...}, ...]

Submitted (done=True, reward=0.0)

============================================================
Episode complete
  Question: What was the total revenue for fiscal year 2023?
  Reward:   0.0
  Steps:    2
============================================================

Cleaning up sandbox...
Done.
```

### 4. Understanding the Training Code

The `train.py` script (~1800 lines) implements end-to-end GRPO training with parallel rollout collection across hundreds of sandboxes. Let's walk through its key components.

#### System Prompt

The model is instructed to act as a financial analyst, using tools iteratively to gather data before answering:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    SYSTEM_PROMPT = """\
    You are a financial analyst assistant answering questions about SEC 10-K filings.

    Think and reason step by step. Iteratively gather data using the available tools
    until you have enough information to answer the question.

    When submitting your final answer:
    - Provide ONLY the numerical value. No explanations, units, or LaTeX formatting.
    - Always express percentages as decimal ratios (e.g., 22% -> 0.22)
    - Submit numbers exactly as they appear in the query results.
    - For multi-year answers, use: year: value, year: value
    - If the question is yes/no, answer Yes or No"""
    ```
  </TabItem>
</Tabs>

#### Tool Schema Fetching

Tool schemas are fetched dynamically from a connected environment via MCP JSON-RPC over WebSocket, then converted to OpenAI function-calling format for use with the chat template:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    async def fetch_tools_from_env(env: FinQAEnv) -> list[dict]:
        resp = await env._send_and_receive(
            {
                "type": "mcp",
                "data": {
                    "jsonrpc": "2.0",
                    "method": "tools/list",
                    "params": {},
                    "id": 1,
                },
            }
        )
        mcp_tools = resp["data"]["result"]["tools"]
        # Convert each tool to OpenAI function-calling format
        openai_tools = []
        for t in mcp_tools:
            schema = t.get("inputSchema") or t.get("input_schema") or {}
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": t["name"],
                    "description": t.get("description", ""),
                    "parameters": {
                        "type": "object",
                        "properties": {
                            name: {"type": prop.get("type", "string"),
                                   "description": prop.get("description", "")}
                            for name, prop in schema.get("properties", {}).items()
                        },
                        "required": schema.get("required", []),
                    },
                },
            })
        return openai_tools
    ```
  </TabItem>
</Tabs>

#### Sandbox Pool Management

The training creates hundreds of sandboxes upfront from the pre-built snapshot, with staggered launches to stay under API rate limits:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    async def create_sandbox_pool(
        n: int, snapshot_name: str, semaphore: asyncio.Semaphore
    ):
        pool_by_idx: list[tuple | None] = [None] * n

        async def create_one(idx: int):
            async with semaphore:
                provider = DaytonaProvider(auto_stop_interval=0, cmd=SERVER_CMD)
                url = await asyncio.to_thread(
                    provider.start_container, f"snapshot:{snapshot_name}"
                )
                for attempt in range(3):
                    try:
                        await asyncio.to_thread(provider.wait_for_ready, url, 120)
                        break
                    except Exception:
                        if attempt == 2:
                            raise
                        await asyncio.sleep(3)
                pool_by_idx[idx] = (provider, url)

        # Stagger launches (10 at a time with 1s sleep) to stay under rate limits
        tasks = []
        for i in range(n):
            tasks.append(asyncio.create_task(create_one(i)))
            if (i + 1) % 10 == 0:
                await asyncio.sleep(1.0)
        await asyncio.gather(*tasks, return_exceptions=True)
        return [entry for entry in pool_by_idx if entry is not None]
    ```
  </TabItem>
</Tabs>

After creation, persistent WebSocket connections are opened to all sandboxes with extended ping timeouts to survive long vLLM generation steps:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    async def connect_envs(pool, play_sem: asyncio.Semaphore) -> list[FinQAEnv]:
        envs: list[FinQAEnv | None] = [None] * len(pool)

        async def connect_one(i: int, url: str):
            async with play_sem:
                env = FinQAEnv(base_url=url)
                await env.connect()
                # Extend ping timeout to survive long vLLM generation steps
                if hasattr(env, "_ws") and env._ws is not None:
                    env._ws.ping_timeout = 300
                envs[i] = env

        await asyncio.gather(
            *[connect_one(i, url) for i, (_, url) in enumerate(pool)]
        )
        return [env for env in envs if env is not None]
    ```
  </TabItem>
</Tabs>

With 500 long-lived WebSocket connections, some will inevitably go stale mid-training (network blips, server-side timeouts, etc.). The `reconnect_envs` function runs a periodic health-check sweep: it sends a lightweight state ping to every connection, and any socket that doesn't respond within 5 seconds gets closed and replaced. Connections with in-flight episode requests are skipped to avoid WebSocket message interleaving — where a ping response and a step response arrive on the same socket and get delivered to the wrong awaiter:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    async def reconnect_envs(
        envs: list[FinQAEnv], pool, skip_indices: set[int] | None = None,
    ) -> list[FinQAEnv]:
        reconnected = 0
        skip = skip_indices or set()

        async def check_and_reconnect(i: int):
            nonlocal reconnected
            env = envs[i]
            try:
                # Quick health check — if the WS is alive this returns fast
                await asyncio.wait_for(
                    env._send_and_receive({"type": "state"}), timeout=5.0
                )
            except Exception:
                # Connection is dead — close and reopen
                try:
                    await env.close()
                except Exception:
                    pass
                _, url = pool[i]
                new_env = FinQAEnv(base_url=url)
                await new_env.connect()
                if hasattr(new_env, "_ws") and new_env._ws is not None:
                    new_env._ws.ping_timeout = 300
                envs[i] = new_env
                reconnected += 1

        await asyncio.gather(
            *[check_and_reconnect(i) for i in range(len(envs)) if i not in skip],
            return_exceptions=True,
        )
    ```
  </TabItem>
</Tabs>

#### Multi-Turn Rollout Collection

The `collect_rollouts` function is the heart of the training loop (~430 lines). It keeps all sandboxes continuously occupied, using a sophisticated async event loop:

1. **Dynamic refill**: As soon as one episode finishes on a sandbox, a new one starts immediately
2. **Batched vLLM generation**: Episodes waiting for a model response are accumulated and dispatched to vLLM as a single batch for throughput
3. **Tool call parsing**: Model outputs are parsed for tool calls (Hermes-style XML, raw JSON, or bare-answer fallback)
4. **Forced termination**: Episodes exceeding `MAX_EPISODE_STEPS` (default 20) get a forced `submit_answer("unknown")`

The flow for a single episode within the rollout engine:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    # 1. Start an episode on an idle sandbox
    async def start_episode(env_idx: int) -> ActiveEpisode:
        env = envs[env_idx]
        await env.reset()
        state = await env._send_and_receive({"type": "state"})
        question = state["data"]["current_question"]
        company = state["data"]["current_company"]
        chat_history = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Company: {company}\nQuestion: {question}"},
        ]
        return ActiveEpisode(env=env, sandbox_idx=env_idx,
                             chat_history=chat_history, ...)

    # 2. Build prompt and generate with vLLM (batched across all ready episodes)
    prompt_str = build_chat_prompt(tokenizer, ep.chat_history)
    outputs = vllm_model.generate(prompts=prompts, sampling_params=params)

    # 3. Parse tool call from generated text
    tool_name, tool_args = parse_tool_call(generated_text)

    # 4. Execute in the sandbox
    result = await ep.env.step(CallToolAction(tool_name=tool_name, arguments=tool_args))

    # 5. If not done: append to chat history, re-enter ready queue
    # If done or max steps: capture reward, mark sandbox as idle
    ```
  </TabItem>
</Tabs>

The `parse_tool_call` function handles multiple output formats from the model:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    def parse_tool_call(text: str) -> tuple[str, dict]:
        # Pattern 1: Hermes-style XML
        # <tool_call>{"name": "sql_query", "arguments": {"query": "..."}}</tool_call>
        m = re.search(r"<tool_call>\s*(\{.*?\})\s*</tool_call>", text, re.DOTALL)
        if m:
            data = json.loads(m.group(1))
            # ... extract name and arguments

        # Pattern 2: Raw JSON objects
        for data in iter_json_objects(text):
            # ... try to extract from {"name": ..., "arguments": ...} format

        # Pattern 3: Bare answer after </think> tag
        # Pattern 4: Text that looks like a number/short answer
        # Fallback: submit_answer({"answer": "unknown"})
    ```
  </TabItem>
</Tabs>

#### Stale WebSocket Cleanup After Cancellation

When rollout collection reaches its target episode count, it cancels any in-flight tasks (episode starts, step requests, forced terminations). But cancellation creates a subtle problem: cancelled tasks leave stale responses queued on their WebSocket connections. If the next iteration reuses that socket, a step response could pick up a stale message from a cancelled task, corrupting the episode.

To prevent this, the code tracks which envs had in-flight WebSocket requests at cancellation time, then force-disconnects those specific sockets. The next `reconnect_envs()` call reopens them cleanly:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    # Cancel excess in-flight work once target sample count is reached
    pending_cancel = list(start_tasks.keys()) + list(step_tasks.keys()) + list(force_tasks.keys())

    # Track envs with in-flight WS requests — cancellation leaves stale
    # responses queued on the socket, corrupting subsequent communication
    stale_env_indices = set()
    for env_idx in start_tasks.values():
        stale_env_indices.add(env_idx)
    for meta in step_tasks.values():
        stale_env_indices.add(meta[0].sandbox_idx)
    for fep in force_tasks.values():
        stale_env_indices.add(fep.sandbox_idx)

    for task in pending_cancel:
        task.cancel()
    await asyncio.gather(*pending_cancel, return_exceptions=True)

    # Force-disconnect envs whose WebSocket has stale responses from
    # cancelled tasks. The next reconnect_envs() will reopen them cleanly.
    for idx in stale_env_indices:
        try:
            await envs[idx].disconnect()
        except Exception:
            pass
    ```
  </TabItem>
</Tabs>

#### GRPO: Grouping and Advantages

Episodes are grouped by the same question (identified by `question_id`). Groups must be exact size (default 6). Leftover episodes carry over to the next iteration:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    def build_strict_prompt_groups(
        episodes: list[Episode], group_size: int
    ) -> tuple[list[list[Episode]], list[Episode]]:
        buckets: dict[tuple[str, str], list[Episode]] = defaultdict(list)
        for ep in episodes:
            buckets[episode_prompt_key(ep)].append(ep)

        groups, leftovers = [], []
        for bucket in buckets.values():
            n_full = len(bucket) // group_size
            for i in range(n_full):
                groups.append(bucket[i * group_size : (i + 1) * group_size])
            leftovers.extend(bucket[n_full * group_size :])
        return groups, leftovers


    def compute_group_advantages(groups: list[list[Episode]]) -> list[list[float]]:
        all_advantages = []
        for group in groups:
            rewards = np.array([ep.reward for ep in group])
            std = float(np.std(rewards))
            if len(group) > 1 and std > 1e-8:
                mean = float(np.mean(rewards))
                advs = (rewards - mean) / (std + 1e-8)
            else:
                advs = np.zeros_like(rewards)  # No gradient signal
            all_advantages.append([float(a) for a in advs])
        return all_advantages
    ```
  </TabItem>
</Tabs>

Within each group, advantages are computed as standard GRPO normalization: `(reward - mean) / std`. If all episodes in a group got the same reward, advantages are zero (no gradient signal from that group).

#### GRPO Policy Gradient Update

The update processes each episode's turns as individual training samples. The loss per turn is `-(advantage * policy_logprob)`:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    def grpo_update(
        train_model, optimizer, episodes_flat, advantages_flat, batch_size=12
    ) -> float:
        train_model.train()
        optimizer.zero_grad(set_to_none=True)

        # Flatten episodes into turn-level samples, sorted by length
        # for efficient padding
        for start in range(0, len(turn_samples), batch_size):
            chunk = turn_samples[start : start + batch_size]

            # Pad and create attention masks
            outputs = train_model(input_ids=input_t, attention_mask=attn_mask)

            # Extract completion logprobs
            nll = F.cross_entropy(completion_logits, comp_targets, reduction="none")
            policy_lps = -nll

            # GRPO loss: -(advantage * logprob) weighted by token count
            token_loss = (-adv_t * policy_lps) * valid_mask
            batch_loss = (token_loss * scale_t).sum()
            batch_loss.backward()

        torch.nn.utils.clip_grad_norm_(train_model.parameters(), max_norm=1.0)
        optimizer.step()
    ```
  </TabItem>
</Tabs>

#### LoRA Hot-Swap

After each training iteration, the updated LoRA adapter is exported and loaded into vLLM for the next rollout. This ensures rollouts always use the freshly updated policy:

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    def export_lora_adapter(train_model, export_root, iteration) -> str:
        out_dir = os.path.join(export_root, f"iter_{iteration:04d}")
        train_model.save_pretrained(out_dir)
        return out_dir

    # In the training loop:
    if new_lora_dir:
        active_lora_request = lora_request_cls(
            f"grpo_iter_{it + 1}", lora_request_seq, new_lora_dir
        )
        # Future vLLM generations use the new adapter automatically
    ```
  </TabItem>
</Tabs>

#### GPU Layout and Lag-1 Pipeline

The training uses a 4-GPU setup with clear separation:
- **GPUs 0-1**: vLLM with tensor parallelism (TP=2) for fast batched generation during rollouts
- **GPUs 2-3**: Base model + LoRA with `device_map="auto"` for training

The training loop overlaps iteration N's gradient update (on GPUs 2-3) with iteration N+1's rollout collection (on GPUs 0-1):

<Tabs>
  <TabItem label="Python" icon="seti:python">
    ```python
    for it in range(args.iterations):
        batch = prepared_batch

        # 1. Start GRPO update on a background thread (uses GPUs 2-3)
        update_task = asyncio.create_task(
            asyncio.to_thread(
                run_grpo_update_and_maybe_export,
                train_model, optimizer, batch, ...
            )
        )

        # 2. While train GPUs are busy, prepare next batch (uses GPUs 0-1)
        if it + 1 < args.iterations:
            prepared_batch = await prepare_train_batch(
                envs=envs, pool=pool, vllm_model=vllm_model, ...
            )

        # 3. Await the update, hot-swap LoRA adapter into vLLM
        loss, new_lora_dir = await update_task
        if new_lora_dir:
            active_lora_request = lora_request_cls(
                f"grpo_iter_{it + 1}", lora_request_seq, new_lora_dir
            )
    ```
  </TabItem>
</Tabs>

### 5. Running the Training

Start training with:

```bash
python train.py
```

For a quick smoke test with minimal resources:

```bash
python train.py --sandboxes 2 --iterations 1 --group-size 2
```

You'll see output like:

```
Creating 500 sandboxes from snapshot 'openenv-finqa' ...
All 500 sandboxes ready.

Connecting to sandboxes ...
All 500 connections ready.

Tools: ['get_descriptions', 'get_table_info', 'sql_query', 'submit_answer']

  iter   accuracy   avg_steps       loss   groups     eps/s     time
------------------------------------------------------------------------
  1/10      0.082        8.3     0.0234       100     12.5     480s
  2/10      0.117        7.9     0.0198       100     13.1     458s
  ...
```

After training completes, artifacts are saved to `runs/YYYYMMDD_HHMMSS/`:
- `config.json` — Full training configuration
- `metrics.jsonl` — Per-iteration metrics (accuracy, loss, eps/sec, etc.)
- `rollouts.jsonl` — Per-round rollout summaries
- `trajectories.jsonl` — Every episode with all turns (tool calls, results, reward)

### 6. Training Results

:::note[Work in Progress]
Training results and a reward curve plot will be added once a full training run completes.
:::

{/* When training results are available, uncomment the Image import at the top and add:
<Image
  src={rewardsPlot}
  alt="Rewards over training steps showing improvement"
  width={700}
  style="max-width: 100%; height: auto; margin: 1rem 0;"
/>
*/}

### 7. Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--sandboxes` | 500 | Number of concurrent Daytona sandboxes |
| `--iterations` | 10 | Training iterations |
| `--group-size` | 6 | Episodes per prompt group for GRPO |
| `--target-groups-per-iter` | 100 | Target number of complete groups per iteration |
| `--max-rollout-rounds` | 8 | Max rollout rounds per iteration |
| `--snapshot` | `openenv-finqa` | Daytona snapshot name |
| `--model` | `Qwen/Qwen3-14B` | HuggingFace model ID |
| `--lr` | 8e-5 | Learning rate |
| `--temperature` | 1.0 | Sampling temperature |
| `--max-steps` | 20 | Max episode steps before forced termination |
| `--max-gen-tokens` | 512 | Max tokens per generation |
| `--tensor-parallel-size` | 2 | vLLM tensor parallelism |
| `--gpu-memory-utilization` | 0.85 | vLLM GPU memory fraction |
| `--lora-rank` | 16 | LoRA rank |
| `--lora-alpha` | 32 | LoRA alpha |
| `--sync-every` | 1 | Export LoRA adapter every N iterations |
| `--grpo-update-batch-size` | 12 | Micro-batch size for GRPO updates |

:::tip[Scaling Tips]
- For a quick smoke test, use `--sandboxes 2 --iterations 1 --group-size 2`
- Increase `--max-steps` for questions that require more exploration
- Decrease `--grpo-update-batch-size` if you run out of GPU memory during the update step
- Keep `--group-size` at 6+ for meaningful advantage estimation within groups
:::

---

**Key advantages of this approach:**

- **Multi-turn tool use**: Agents learn to iteratively explore and query financial data across multiple steps
- **Massive parallelism**: Hundreds of sandboxes collect episodes simultaneously
- **Safe execution**: SQL queries and data exploration execute in isolated environments
- **OpenEnv protocol**: Standard RL environment interface over WebSocket, decoupling the environment from the agent
- **LoRA efficiency**: Parameter-efficient training with hot-swappable adapters avoids full model copies
- **Lag-1 pipeline**: Overlapped rollout collection and gradient updates maximize GPU utilization
