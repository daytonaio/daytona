# Runner-Android Implementation Status

Cuttlefish (Android Virtual Device) runner for Daytona - implementation status and roadmap.

## Overview

`runner-android` is a Cuttlefish-based runner designed for Android virtual devices with WebRTC display streaming, ADB-based device management, and multi-instance support.

**Hypervisor:** crosvm (via Cuttlefish/CVD)
**Target Host:** Bare-metal Linux x86_64 with KVM support
**Current Test Host:** `40.160.30.187` (32 cores, 125GB RAM, 878GB storage)

## âœ… Completed Features

### Core Infrastructure

- [x] Cuttlefish CVD client (create, start, stop, destroy via `cvd` CLI)
- [x] **Dual-mode operation: Local + Remote (SSH)**
  - Local mode: Runner on same host as Cuttlefish (production)
  - Remote mode: Runner connects via SSH (development)
- [x] API server with Gin framework
- [x] Authentication middleware
- [x] Prometheus metrics endpoint
- [x] Instance mapping persistence (`mappings.json`)

### VM Lifecycle

- [x] Create sandbox (`cvd create` with configurable CPUs, memory, snapshot)
- [x] Start sandbox (`cvd start` for stopped instances, re-create as fallback)
- [x] Stop sandbox (`cvd stop` by group name)
- [x] Destroy sandbox (`cvd rm` + process cleanup + directory cleanup)
- [x] Get sandbox info (state, ADB port, metadata)
- [x] **Multi-layered health detection** (CVD fleet â†’ ADB liveness â†’ crosvm process)
- [x] **Stale VM recovery** (handles CVD "Cancelled" status after server restarts)

### ADB Integration (instead of SSH)

All device interactions use ADB (Android Debug Bridge) instead of SSH:

- [x] ADB shell command execution
- [x] ADB file push/pull
- [x] ADB port forwarding info
- [x] ADB device discovery and serial management
- [x] Per-instance ADB ports (base port 6520 + instance_num - 1)

### WebRTC Display Streaming

- [x] **Cuttlefish WebRTC operator** integration (port 1443 HTTPS)
- [x] **WebRTC proxy** from runner API to operator
- [x] **Clean URL support** (`/` serves `client.html` without redirect)
  - Injects `window.__DAYTONA_DEVICE_ID` for device identification
  - Intercepts and modifies `server_connector.js` for clean URL compatibility
  - `<base>` tag injection for correct asset loading
- [x] **WebSocket proxying** for WebRTC signaling
- [x] **Device ID auto-discovery** from operator `/devices` endpoint
- [x] Port 6080 proxy routing (per-sandbox WebRTC access)

### Toolbox API (Partial â€” via ADB)

Toolbox commands are implemented via ADB shell instead of a daemon inside the VM.

**Supported commands:**

| Endpoint | Method | Status | Notes |
|----------|--------|--------|-------|
| `process/execute` | POST | âœ… | Execute shell command via `adb shell` |
| `process/commands/{id}` | GET | âŒ | Not supported (commands are synchronous) |
| `files` | GET | âœ… | List files via `adb shell ls` |
| `files/info` | GET | âœ… | File info via `adb shell stat` |
| `files/download` | GET | âœ… | Download via `adb pull` |
| `files/upload` | POST | âœ… | Upload via `adb push` |
| `files/folder` | POST | âœ… | Create folder via `adb shell mkdir` |
| `files/move` | POST | âœ… | Move/rename via `adb shell mv` |
| `files` | DELETE | âœ… | Delete via `adb shell rm` |
| `git/*` | ANY | âŒ | Not supported on Android |
| `workspace` | GET | âœ… | Returns `/sdcard` as default workspace |
| `computeruse/status` | GET | âœ… | WebRTC availability status |
| `computeruse/screenshot` | GET/POST | âœ… | Screenshot via `adb shell screencap` |
| `computeruse/keyboard/type` | POST | âœ… | Text input via `adb shell input text` |
| `computeruse/keyboard/key` | POST | âœ… | Key press via `adb shell input keyevent` |
| `computeruse/mouse/click` | POST | âœ… | Tap via `adb shell input tap` |
| `computeruse/mouse/move` | POST | âœ… | Move cursor |
| `computeruse/mouse/drag` | POST | âœ… | Swipe via `adb shell input swipe` |
| `computeruse/mouse/scroll` | POST | âœ… | Scroll via `adb shell input roll` |

### Android-Specific Endpoints

| Endpoint | Method | Status | Notes |
|----------|--------|--------|-------|
| `/sandboxes/:id/adb/info` | GET | âœ… | ADB port, serial, tunnel command |
| `/sandboxes/:id/android/install` | POST | âœ… | Install APK (multipart or base64) |
| `/sandboxes/:id/android/uninstall` | POST | âœ… | Uninstall app by package name |
| `/sandboxes/:id/android/packages` | GET | âœ… | List installed packages |
| `/sandboxes/:id/android/launch` | POST | âœ… | Launch app/activity |
| `/sandboxes/:id/android/stop` | POST | âœ… | Force stop app |
| `/sandboxes/:id/android/props` | GET | âœ… | Get system properties |
| `/sandboxes/:id/android/logcat` | GET | âœ… | Stream logcat via SSE |
| `/sandboxes/:id/android/device` | GET | âœ… | Device info (model, version, SDK) |

### Health Monitoring

- [x] **Multi-layered instance state detection:**
  1. CVD fleet status (`cvd fleet --json`) â€” fast but can show stale "Cancelled"
  2. ADB liveness (`adb shell getprop sys.boot_completed`) â€” ground truth
  3. crosvm process detection (`ps -eo args | grep crosvm`) â€” fallback
- [x] **Crash detection** with configurable retry count before reporting
- [x] **Automatic crash reporting** to Daytona API
- [x] **CVD state synchronization** (removes orphaned CVD instances)
- [x] **Operator device cleanup** (removes stale cuttlefish-operator registrations)

### Metrics & Healthcheck

- [x] Remote metrics collection via SSH (CPU, RAM, disk)
- [x] Allocated resources tracking
- [x] Healthcheck service (reports to Daytona API)
- [x] Snapshot count tracking

### Job System (v2 API)

- [x] Job poller service
- [x] Job executor with OpenTelemetry tracing
- [x] CREATE_SANDBOX job handler
- [x] START_SANDBOX job handler
- [x] STOP_SANDBOX job handler
- [x] DESTROY_SANDBOX job handler

### Snapshots & S3

- [x] Snapshot existence check (local directory-based)
- [x] Base image support (Cuttlefish system images as snapshots)
- [x] Custom snapshot support (org-scoped, `{orgId}/{snapshotName}` format)
- [x] Snapshot directory management
- [x] **Create snapshot from running VM** â€” copies per-instance disk files (overlay.img, sdcard.img, etc.)
- [x] **S3 upload** â€” auto-uploads snapshot to S3 after local creation
- [x] **S3 download** â€” pulls snapshot from S3 if not present locally
- [x] **S3 existence check** â€” checks S3 for snapshot availability
- [x] **S3 delete** â€” removes snapshot from S3
- [x] Remote file streaming (SSH pipe to S3 for remote mode)
- [x] Symlink resolution (follows symlinks before uploading to S3)

**Snapshot structure (local):**

```
/var/lib/cuttlefish/artifacts/snapshots/{orgId}/{snapshotName}/
â”œâ”€â”€ manifest.json       # Metadata (name, org, base image, creation time, size)
â”œâ”€â”€ instance_data/      # Per-instance disk files (the actual VM state)
â”‚   â”œâ”€â”€ overlay.img     # System overlay (~550 MB) â€” installed apps, system changes
â”‚   â”œâ”€â”€ sdcard.img      # SD card data (2 GB sparse)
â”‚   â”œâ”€â”€ metadata.img    # Metadata partition (64 MB)
â”‚   â”œâ”€â”€ pflash.img      # UEFI/bootloader vars (3 MB)
â”‚   â”œâ”€â”€ uboot_env.img   # U-Boot environment (72 KB)
â”‚   â”œâ”€â”€ misc.img        # Misc partition (1 MB)
â”‚   â””â”€â”€ ap_overlay.img  # AP overlay (10 MB)
â”œâ”€â”€ super.img â†’ (symlink to base)
â”œâ”€â”€ boot.img â†’ (symlink to base)
â””â”€â”€ ... other base image symlinks
```

**S3 structure:**

```
s3://{bucket}/snapshots/{orgId}/{snapshotName}/
â”œâ”€â”€ manifest.json
â”œâ”€â”€ instance_data/overlay.img
â”œâ”€â”€ instance_data/sdcard.img
â”œâ”€â”€ instance_data/metadata.img
â”œâ”€â”€ ... (all files, symlinks resolved to real content)
â”œâ”€â”€ super.img        # Full base image (~1.9 GB)
â”œâ”€â”€ boot.img         # Kernel (64 MB)
â””â”€â”€ ... all base images (self-contained for cross-runner restore)
```

**Known snapshot limitations:**

| Limitation | Cause | Impact |
|-----------|-------|--------|
| No memory state capture | crosvm virtio-fs doesn't support `virtio_sleep` | VM must cold-boot from snapshot (no instant restore) |
| CVD overlay path-specific | Overlays reference assembly paths internally | Overlay files backed up for DR but can't directly replace on new instance |
| Cold boot on restore | Custom snapshots launch from base images | ~60-120s boot time, same as fresh VM |
| No `--snapshot_compat` | `cvd_internal_start` doesn't recognize the flag | Can't use CVD's built-in snapshot_take/restore |

### Disk Management

- [x] CoW overlay images (only deltas stored per instance, ~570 MiB per VM)
- [x] Shared base artifacts (3.2 GiB shared across all VMs)
- [x] Configurable virtual disk size (default 20 GiB)

## ğŸš§ Partially Implemented

### Snapshots & Images

- [x] Pull snapshot from S3 (downloads if not present locally)
- [x] Push snapshot to S3 (auto-uploads on create)
- [ ] Restore VM state from snapshot disk files (overlay replacement after CVD assembly)
- [ ] Build snapshot from config (stub)
- [ ] Tag image (stub)

### SSH Gateway

- [x] SSH gateway service structure
- [ ] Full SSH-to-ADB bridging (partially implemented)

### Memory Ballooning

- [x] `crosvm_use_balloon: true` enabled in Cuttlefish config
- [x] `--balloon` flag passed to crosvm
- [ ] **Guest driver not available** â€” `CONFIG_VIRTIO_BALLOON=m` but `.ko` not shipped in stock images
- [ ] Balloon control via `crosvm balloon <bytes> <socket>` â€” works host-side but no guest response
- [ ] Requires custom Android kernel build with `CONFIG_VIRTIO_BALLOON=y`

## âŒ Not Yet Implemented

### High Priority

- [ ] **Warm snapshot restore** (blocked by crosvm virtio-fs `virtio_sleep` limitation)
- [ ] **Overlay restore from snapshot** (replace CVD overlays with backed-up files post-assembly)
- [ ] Memory ballooning (requires custom kernel with `CONFIG_VIRTIO_BALLOON=y`)

### Performance Optimizations

- [ ] VM pool (pre-warmed instances for instant creation)
- [ ] Snapshot restore (fast boot from memory state)
- [ ] Huge pages support

### Advanced Features

- [ ] Live resize (CPU/memory hotplug)
- [ ] GPU passthrough (Cuttlefish supports GPU acceleration)
- [ ] Live migration
- [ ] Nested virtualization

## Configuration

### Environment Variables

```bash
# Required
SERVER_URL=http://localhost:3000      # Daytona API URL
API_TOKEN=<token>                      # Runner API token

# Cuttlefish Host (remote mode only)
# Leave CVD_SSH_HOST empty for local mode
CVD_SSH_HOST=root@40.160.30.187
CVD_SSH_KEY_PATH=/path/to/id_rsa

# Optional
API_PORT=3107                          # Runner API port
LOG_LEVEL=info                         # debug, info, warn, error
RUNNER_DOMAIN=127.0.0.1                # Runner domain for API registration

# VM Defaults
CVD_DEFAULT_CPUS=4                     # vCPUs per VM
CVD_DEFAULT_MEMORY_MB=4096             # Memory per VM (MB)
CVD_DEFAULT_DISK_GB=20                 # Virtual disk size (GB)

# Cuttlefish Paths
CVD_INSTANCES_PATH=/var/lib/cuttlefish/instances
CVD_ARTIFACTS_PATH=/var/lib/cuttlefish/artifacts
CVD_HOME=/home/vsoc-01
CVD_PATH=/usr/bin/cvd
CVD_ADB_PATH=/usr/lib/cuttlefish-common/bin/adb
CVD_ADB_BASE_PORT=6520
CVD_MAX_INSTANCES=100

# SSH Gateway (optional)
SSH_GATEWAY_ENABLE=false

# S3 Configuration (for snapshot push/pull)
AWS_ENDPOINT_URL=https://s3.us-east-2.amazonaws.com
AWS_ACCESS_KEY_ID=<key>
AWS_SECRET_ACCESS_KEY=<secret>
AWS_REGION=us-east-2
AWS_DEFAULT_BUCKET=<bucket-name>
```

### Host Requirements

```bash
# Cuttlefish packages
sudo apt install cuttlefish-base cuttlefish-user

# User setup
sudo usermod -aG kvm,cvdnetwork,render vsoc-01

# TAP interface provisioning (default: 10, increase for more VMs)
# Edit /etc/default/cuttlefish-host-resources
num_cvd_accounts=20
sudo systemctl restart cuttlefish-host-resources

# Verify
cvd version
adb devices
```

### Directory Structure (Host)

```
/var/lib/cuttlefish/
â”œâ”€â”€ artifacts/             # Cuttlefish system images
â”‚   â””â”€â”€ android-LATEST/    # Base image (symlink to cf_vm)
â”œâ”€â”€ instances/             # Runner instance data
â”‚   â”œâ”€â”€ mappings.json      # sandboxId â†” instanceNum mappings
â”‚   â””â”€â”€ <sandbox-id>/      # Per-sandbox data
â”‚       â””â”€â”€ instance.json  # Instance configuration
â””â”€â”€ snapshots/             # Custom snapshots (orgId/name)

/var/tmp/cvd/<uid>/        # CVD runtime (managed by cvd)
â”œâ”€â”€ <run-id>/
â”‚   â”œâ”€â”€ home/cuttlefish/
â”‚   â”‚   â”œâ”€â”€ assembly/      # Assembled config
â”‚   â”‚   â””â”€â”€ instances/cvd-N/
â”‚   â”‚       â”œâ”€â”€ overlay.img   # CoW overlay (~545 MiB)
â”‚   â”‚       â”œâ”€â”€ sdcard.img    # SD card (2 GiB sparse)
â”‚   â”‚       â””â”€â”€ logs/         # Instance logs
â”‚   â””â”€â”€ artifacts/         # Symlinks to base images
â””â”€â”€ instance_database.binpb  # CVD instance registry

/tmp/cf_avd_<uid>/cvd-N/internal/
â””â”€â”€ crosvm_control.sock    # crosvm control socket
```

## Resource Limits

### Per-VM Resource Usage

| Resource | Allocated | Actual Host Usage |
|----------|-----------|-------------------|
| CPU | 4 vCPUs (configurable) | Shared with host |
| Memory | 4 GiB (configurable) | ~4 GiB RSS per crosvm process |
| Disk (runtime) | 20 GiB virtual | ~570 MiB actual (CoW overlay) |
| Disk (shared base) | â€” | 3.2 GiB (shared across all VMs) |
| Network | 2 TAP interfaces per VM | etap + mtap |
| ADB port | 1 per VM (6520 + N-1) | â€” |
| vsock CID | 1 per VM | 32-bit space |

### Host Scaling Limits

| Limit | Default | How to Increase |
|-------|---------|----------------|
| **TAP interfaces** | 10 VMs | Set `num_cvd_accounts=N` in `/etc/default/cuttlefish-host-resources` |
| **RAM** | ~6-7 VMs (at 4 GiB each on 125 GiB host) | Add RAM, reduce per-VM allocation |
| **CPU** | 8 VMs (at 4 vCPUs on 32 cores, no overcommit) | CPU overcommit works for idle VMs |
| **Disk** | Hundreds of VMs | Not a practical concern |

## Known Issues

1. **CVD "Cancelled" status** â€” After runner restart, `cvd fleet` shows "Cancelled" for running VMs. The multi-layered health check handles this by falling back to ADB and process detection.
2. **Memory ballooning not functional** â€” Guest kernel has `CONFIG_VIRTIO_BALLOON=m` but the `.ko` module is not shipped in stock Cuttlefish images. Requires custom kernel build.
3. **crosvm balloon control fragile** â€” `crosvm balloon_stats` can hang or kill VMs if guest driver is not loaded.
4. **Operator device registration stale** â€” After `cvd rm`, the cuttlefish-operator may retain stale device entries. Runner proactively cleans these.
5. **No idle timeout in Cuttlefish** â€” VMs run indefinitely; no built-in auto-shutdown for idle devices.
6. **WebRTC vsock connection resets** â€” Occasional `vsock_connection.cpp: Failed to connect: Connection reset by peer` in WebRTC logs, causing temporary display disconnects.
7. **Snapshot restore is cold-boot only** â€” Custom snapshots create a fresh VM from the base images (cold boot). The backed-up disk files (overlay, sdcard) are stored in `instance_data/` for disaster recovery but not yet automatically restored into the new instance. This is because CVD overlays are path-specific and get wiped when base image paths change.
8. **crosvm virtio-fs blocks suspend/snapshot** â€” `cvd snapshot_take` and `cvd suspend` fail with `virtio_sleep not implemented for virtio-fs`. This blocks CVD's native memory+disk snapshot mechanism. A newer crosvm version may fix this.
9. **Stale CVD sockets cause boot failures** â€” Failed `cvd create` attempts leave stale sockets in `/tmp/cf_avd_*` that cause subsequent `run_cvd` processes to crash with `Failed to read a complete exit code`. Cleaning `/tmp/cf_avd_*` and `/tmp/cf_env_*` resolves this.

## Comparison with runner-ch

| Feature | runner-ch (Cloud Hypervisor) | runner-android (Cuttlefish) |
|---------|-----------------------------|-----------------------------|
| Hypervisor | Cloud Hypervisor | crosvm (via CVD) |
| OS Support | Linux only | Android only |
| Display | VNC/SPICE (planned) | âœ… WebRTC (built-in) |
| Shell Access | SSH | ADB |
| GPU Passthrough | ğŸš§ (untested) | âœ… (Cuttlefish GPU 2D) |
| Memory Ballooning | âœ… | âŒ (guest driver missing) |
| Live Fork | âœ… (local mode) | âŒ |
| Warm Snapshots | âœ… (~4s restore) | âŒ (blocked by virtio-fs) |
| Disk Snapshots to S3 | âœ… | âœ… (overlay + sdcard + metadata) |
| Boot Time | ~5-10s (cold), ~4s (warm) | ~60-120s |
| Memory Overhead | Lower | Higher (~18 GiB RSS for 4 GiB VM) |
| Disk Format | qcow2 (CoW) | qcow2-like (CVD overlay) |
| Per-VM Disk | ~20 MiB overlay | ~570 MiB overlay |
| Computer Use | âœ… (via daemon) | âœ… (via ADB + WebRTC) |
| File Operations | âœ… (via daemon) | âœ… (via ADB push/pull) |
| Process Execution | âœ… (via daemon) | âœ… (via ADB shell) |

## Architecture

### VM Management Flow

```
Daytona API â†’ Runner API (port 3107) â†’ CVD Client â†’ cvd CLI â†’ crosvm
                  â”‚                         â”‚
                  â”‚                         â”œâ”€â”€ ADB Client (shell, push, pull)
                  â”‚                         â”œâ”€â”€ Health Monitor (CVD + ADB + process)
                  â”‚                         â””â”€â”€ Instance Mapper (sandboxId â†” instanceNum)
                  â”‚
                  â”œâ”€â”€ WebRTC Proxy â†’ Cuttlefish Operator (port 1443)
                  â””â”€â”€ Toolbox Handler â†’ ADB Shell
```

### WebRTC Proxy Flow

```
Browser â†’ Proxy (port 4000) â†’ Runner API (port 3107) â†’ Cuttlefish Operator (port 1443)
   â”‚                                â”‚
   â”‚  6080-{sandboxId}.proxy.localhost:4000/
   â”‚                                â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚  â”‚
   â”‚  â”œâ”€â”€ / â†’ Fetch client.html, inject device ID + base tag
   â”‚  â”œâ”€â”€ /js/server_connector.js â†’ Modify deviceId() to use injected ID
   â”‚  â”œâ”€â”€ /js/*.js, /style.css â†’ Proxy to operator with device path prefix
   â”‚  â”œâ”€â”€ /infra_config â†’ Pass through to operator
   â”‚  â””â”€â”€ /polled_connections â†’ Pass through to operator (WebSocket)
```

### Health Check Flow

```
Health Monitor (every 30s)
    â”‚
    â”œâ”€â”€ Layer 1: cvd fleet --json
    â”‚   â”œâ”€â”€ "Running" â†’ âœ… Trust it
    â”‚   â”œâ”€â”€ "Stopped" â†’ âœ… Trust it
    â”‚   â””â”€â”€ "Cancelled" or missing â†’ âš ï¸ Don't trust, check Layer 2
    â”‚
    â”œâ”€â”€ Layer 2: adb shell getprop sys.boot_completed
    â”‚   â”œâ”€â”€ "1" â†’ âœ… VM is alive (CVD metadata is stale)
    â”‚   â””â”€â”€ No response â†’ Check Layer 3
    â”‚
    â””â”€â”€ Layer 3: ps -eo args | grep crosvm.*cvd-N
        â”œâ”€â”€ Found â†’ âš ï¸ VM is booting (ADB not ready yet)
        â””â”€â”€ Not found â†’ âŒ VM is truly dead, report crash
```

---

_Last updated: 2026-02-08_
